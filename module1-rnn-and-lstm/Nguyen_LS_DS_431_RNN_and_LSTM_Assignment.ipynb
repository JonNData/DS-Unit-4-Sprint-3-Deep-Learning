{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S3-DNN (Python 3.7)",
      "language": "python",
      "name": "u4-s3-dnn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Nguyen_LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonNData/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/module1-rnn-and-lstm/Nguyen_LS_DS_431_RNN_and_LSTM_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZjsefHXDjED",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "colab": {}
      },
      "source": [
        "# TODO - Words, words, mere words, no matter from the heart.\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import requests\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMljU_nbFEOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://www.gutenberg.org/files/100/100-0.txt'\n",
        "r = requests.get(url)\n",
        "r.encoding = r.apparent_encoding\n",
        "\n",
        "data = r.text\n",
        "\n",
        "data = data.split('\\r\\n')\n",
        "\n",
        "data = data[135:]\n",
        "\n",
        "sonets = data[:2776]\n",
        "plays = data[2777:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHvY1nLAFlQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def long_lines(lst_ln):\n",
        "  \"\"\" Return long cleaned lines \"\"\"\n",
        "  clean = []\n",
        "  for ln in lst_ln:\n",
        "    if len(ln) == 0:\n",
        "      pass\n",
        "    else: # if the characters are mostly letters, add it to the list\n",
        "      pct = len(ln.strip(' ')) / len(ln)\n",
        "      if pct >= 0.5:\n",
        "        clean.append(ln.lstrip())\n",
        "  return clean\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3LODTy8HDUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ea55de89-26a6-48b4-8c78-dc8015bb5af4"
      },
      "source": [
        "sonets_c = long_lines(sonets)\n",
        "plays_c = long_lines(plays)\n",
        "plays_c[:5]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ALL’S WELL THAT ENDS WELL',\n",
              " 'Contents',\n",
              " 'ACT I',\n",
              " 'Scene I. Rossillon. A room in the Countess’s palace.',\n",
              " 'Scene II. Paris. A room in the King’s palace.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqkUBciMHJpE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b45aac31-74de-47df-c275-cbeba3491f76"
      },
      "source": [
        "# Character encoding\n",
        "\n",
        "text = \"\\r\\n\".join(plays_c)\n",
        "print(text[1:10])\n",
        "\n",
        "chars = list(set(text))\n",
        "print(chars[:10])\n",
        "len(chars)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LL’S WELL\n",
            "['“', 'v', 'î', ';', '5', 'B', '‘', '|', '’', 's']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "106"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLWRTtb8d__z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c889c4c7-a225-474e-e9bf-2335d0df1d9b"
      },
      "source": [
        "# Create a lookup dictionary that can be referenced for all the chars\n",
        "char_int = {c:i for i,c in enumerate(chars)}\n",
        "int_char = {i:c for i,c in enumerate(chars)}\n",
        "len(chars), len(int_char)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(106, 106)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyEy8iS3TSiV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "a319f978-c6b7-4fdd-904e-78044dcc6ab6"
      },
      "source": [
        "int_char[52]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-6e69b8217142>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mint_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 52"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxCOxhhgmM5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb7faf5f-d44d-489a-b2b8-0bbf30a60e79"
      },
      "source": [
        "# Create the sequence data\n",
        "\n",
        "maxlen = 40\n",
        "step = 5\n",
        "\n",
        "encoded = [char_int[c] for c in text]\n",
        "\n",
        "sequences = [] # Each element is 40 chars long\n",
        "next_char = [] # One element for each sequence\n",
        "# take 40 chars and predict next char. each time\n",
        "\n",
        "# for the length of our total encoded data -  the 40 chars we predicted.\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "    \n",
        "    sequences.append(encoded[i : i + maxlen])\n",
        "    next_char.append(encoded[i + maxlen])\n",
        "    \n",
        "print('sequences: ', len(sequences))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences:  1064489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR7wATtitwe0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "f3098279-02fa-4e3f-83da-42eee63ad31a"
      },
      "source": [
        "sequences[1]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[52,\n",
              " 92,\n",
              " 87,\n",
              " 94,\n",
              " 94,\n",
              " 52,\n",
              " 95,\n",
              " 11,\n",
              " 72,\n",
              " 95,\n",
              " 52,\n",
              " 87,\n",
              " 23,\n",
              " 68,\n",
              " 46,\n",
              " 52,\n",
              " 92,\n",
              " 87,\n",
              " 94,\n",
              " 94,\n",
              " 58,\n",
              " 104,\n",
              " 96,\n",
              " 42,\n",
              " 56,\n",
              " 44,\n",
              " 73,\n",
              " 56,\n",
              " 44,\n",
              " 9,\n",
              " 58,\n",
              " 104,\n",
              " 72,\n",
              " 96,\n",
              " 95,\n",
              " 52,\n",
              " 37,\n",
              " 58,\n",
              " 104,\n",
              " 46]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnippFJ0M-gH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03011c1c-627b-4b4c-a1b2-70c274ecb7e5"
      },
      "source": [
        "len(sequences), len(sequences[1])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1064489, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps1HmuQ1t8fW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we have the sequence data but it is not encoded properly\n",
        "\n",
        "# Since we are only 106 features, we have to one hot encode\n",
        "#  Currently each item is 106 elemnents long, index, sequence, one hot encode\n",
        "\n",
        "# Create x & y, 40 characters in and next_char prediction\n",
        "# make a bunch of zeroes, and make a one for the target\n",
        "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        # i = obs, item in obs, encoded to 1\n",
        "        x[i,t,char] = 1\n",
        "    #  y is the next character    \n",
        "    y[i, next_char[i]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGPbAgcKOtYl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1323e3c6-548b-4ea4-9090-68e159a51391"
      },
      "source": [
        "x.shape, y.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1064489, 40, 106), (1064489, 106))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i2U0W3_xwxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CHI52JHuOp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model: a single LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='nadam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c86X-ZXYxrEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    # takes probabilities and gives location for highest prob\n",
        "    # essentially giving our prediction\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y8eLTkxx21m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx5CsI2Bx8JL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "c2d056dc-2264-4610-9788-673d979fc72a"
      },
      "source": [
        "# fit the model\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=256,\n",
        "          epochs=10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4159/4159 [==============================] - 36s 9ms/step - loss: 2.0114\n",
            "Epoch 2/10\n",
            "4159/4159 [==============================] - 36s 9ms/step - loss: 1.8842\n",
            "Epoch 3/10\n",
            "4159/4159 [==============================] - 36s 9ms/step - loss: 1.8046\n",
            "Epoch 4/10\n",
            "4159/4159 [==============================] - 37s 9ms/step - loss: 1.7473\n",
            "Epoch 5/10\n",
            "4159/4159 [==============================] - 36s 9ms/step - loss: 1.7037\n",
            "Epoch 6/10\n",
            "4159/4159 [==============================] - 36s 9ms/step - loss: 1.6695\n",
            "Epoch 7/10\n",
            "4159/4159 [==============================] - 37s 9ms/step - loss: 1.6417\n",
            "Epoch 8/10\n",
            "4159/4159 [==============================] - 37s 9ms/step - loss: 1.6187\n",
            "Epoch 9/10\n",
            "4159/4159 [==============================] - 37s 9ms/step - loss: 1.5993\n",
            "Epoch 10/10\n",
            "4159/4159 [==============================] - 37s 9ms/step - loss: 1.5824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa3c4651c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr57meBEKXZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(x, y,\n",
        "          batch_size=256,\n",
        "          epochs=1, callbacks=[print_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttm6V8OqUoqW",
        "colab_type": "text"
      },
      "source": [
        "###This is fuckin' crazy, they are mostly words now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97EwIgMXOSRn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b929c40-e40e-4eab-e195-45691ddc70a0"
      },
      "source": [
        "model.fit(x, y,\n",
        "          batch_size=256,\n",
        "          epochs=10, callbacks=[print_callback])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4157/4159 [============================>.] - ETA: 0s - loss: 1.5433\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"my nobler thoughts most base, is now\n",
            "Th\"\n",
            "my nobler thoughts most base, is now\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Which but beheir paster'd; s\n",
            "4159/4159 [==============================] - 45s 11ms/step - loss: 1.5433\n",
            "Epoch 2/10\n",
            "4159/4159 [==============================] - ETA: 0s - loss: 1.5333\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"ive or dead,\n",
            "He will be found like Brut\"\n",
            "ive or dead,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "And se\n",
            "4159/4159 [==============================] - 46s 11ms/step - loss: 1.5333\n",
            "Epoch 3/10\n",
            "4156/4159 [============================>.] - ETA: 0s - loss: 1.5238\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"lie, you lie:\n",
            "I say thou liest, Camillo\"\n",
            "lie, you lie:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "At with his ranging me our dea\n",
            "4159/4159 [==============================] - 47s 11ms/step - loss: 1.5238\n",
            "Epoch 4/10\n",
            "4154/4159 [============================>.] - ETA: 0s - loss: 1.5156\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"e to me.\n",
            "YORK. If York have ill demean'\"\n",
            "e to me.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "That serval take thee? A robser, she is it was with blood hut.\n",
            "4159/4159 [==============================] - 46s 11ms/step - loss: 1.5156\n",
            "Epoch 5/10\n",
            "4157/4159 [============================>.] - ETA: 0s - loss: 1.5077\n",
            "----- Generating text after Epoch: 4\n",
            "\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "And would now returate’s in one ton\n",
            "4159/4159 [==============================] - 46s 11ms/step - loss: 1.5078\n",
            "Epoch 6/10\n",
            "4155/4159 [============================>.] - ETA: 0s - loss: 1.5005\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \" princes to act,\n",
            "And monarchs to behold\"\n",
            " princes to act,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "What be my brother knowfle to the light hav\n",
            "4159/4159 [==============================] - 46s 11ms/step - loss: 1.5007\n",
            "Epoch 7/10\n",
            "4155/4159 [============================>.] - ETA: 0s - loss: 1.4943\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \" acts. Yet if I knew\n",
            "What hoop should h\"\n",
            " acts. Yet if I knew\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CLIVILLE\n",
            "4159/4159 [==============================] - 46s 11ms/step - loss: 1.4942\n",
            "Epoch 8/10\n",
            "4156/4159 [============================>.] - ETA: 0s - loss: 1.4882\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \" is the Duke of Norfolk, gentle Warwick?\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mine.  Y, state charms, Rommed upon to unt\n",
            "4159/4159 [==============================] - 46s 11ms/step - loss: 1.4882\n",
            "Epoch 9/10\n",
            "4155/4159 [============================>.] - ETA: 0s - loss: 1.4827\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"emaining in the coffer of her friends.\n",
            "\"\n",
            "emaining in the coffer of her friends.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Makes he dediest; herarly see they need, \n",
            "4159/4159 [==============================] - 46s 11ms/step - loss: 1.4827\n",
            "Epoch 10/10\n",
            "4155/4159 [============================>.] - ETA: 0s - loss: 1.4772\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \" the chaste, and unexpressive she.      \"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Shall prif\n",
            "4159/4159 [==============================] - 46s 11ms/step - loss: 1.4773\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa3c44fc748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}